\section{Using pbdBASE}
\addcontentsline{toc}{section}{\thesection. Using pbdBASE}

% The use of the package falls into two separate cases:  beginners who are unfamiliar with ScaLAPACK, and those already familiar with ScaLAPACK.

\subsection{BLACS Communicators}
\addcontentsline{toc}{subsection}{\thesubsection. BLACS Communicators}


Briefly, distributed matrix computations using ScaLAPACK require specialized MPI communicators, via the BLACS library.  As with any MPI communicator, you must initialize it before getting started with communications, and you must terminate it when you are finished with communications.  For most users, this will amount to calling

\begin{lstlisting}[language=rr]
library(pbdBASE, quiet = TRUE)
init.grid() # initialize

# ...

finalize() # terminate
\end{lstlisting}

This special communicator may be used with \pkg{pbdMPI} communicator(s) without causing problems, and by default one \code{finalize()} call will terminate all communicators, whether they be from \pkg{pbdMPI} or \pkg{pbdBASE} (see the \pkg{pbdBASE} reference manual for more details and options).  

\subsubsection{Construction}
\addcontentsline{toc}{subsubsection}{\thesubsubsection. Construction}

BLACS communicators are not identical to \pkg{pbdMPI} communicators.  Indeed, while a \pkg{pbdMPI} communicator is a one-dimensional array of processors, BLACS communicators are two-dimensional (row-major) grids.  These values are simply referred to as the number of processor rows and the number of processor columns, as a communicator really is thought of as a matrix of processors.  When a grid is initialized with \code{init.grid()} and no arguments are passed, then three communicators are created.  These grids are referenced by their ``integer context'' value, or \code{ICTXT}.  These grids are numbered 0, 1, and 2.  Context 0 tries to be the ``best possible'' context (see \citep{slug}).  Here we make 2 choices:
\begin{enumerate}
  \item Grids are always as close to square as possible.  
  \item In the event a grid can not be made to be square, the larger value is used for the number of processor rows.
\end{enumerate}

So for example, if we have 4 processors, then by default this would create a $2\times 2$ grid for context 0.  However, if we have 6 processors, then by default this will create a $3\times 2$ grid of processors.

On the other hand, context 1 is always a $1\times n$ grid, where $n$ is the total number of processors.  Likewise, context 2 is always a $n\times 1$ grid of processors.  These can be extremely valuable, especially for performing data movement operations.

The function \code{init.grid()} does a great deal of (useful) hand-holding, so the much more advanced user who is familiar with BLACS may be more interested in the function \code{blacs_gridinit()}, which does not reserve contexts 0, 1, or 2.  However, many \pkg{pbdDMAT} functions make assumptions about the existence and shapes of contexts 0, 1, and 2 (as described above), so this functionality is not supported when using that package.

\subsubsection{Destruction}
\addcontentsline{toc}{subsubsection}{\thesubsubsection. Destruction}

The user can halt all communicators --- both BLACS communicators and, optionally, those created by \pkg{pbdMPI} (or others) --- by calling \code{finalize()}.  To destroy just a single BLACS context (for example, one used to read in data on a subset of processors), then the user should use \code{gridexit()}.  See the \pkg{pbdBASE} reference manual for full details.



\subsection{Notes for Developers}
\addcontentsline{toc}{subsection}{\thesubsection. Notes for Developers}

The \pkg{pbdBASE} package also has several useful routines for package developers who need to deal with distributed matrices (such as \pkg{pbdDMAT}'s \code{ddmatrix} object).  Chief among these is the \code{numroc()} function.  Here, \code{numroc} stands for \textbf{num}ber of \textbf{r}ows \textbf{o}r \textbf{c}olumns.  This routine is used for determining local storage dimensions.  If you need to construct a distributed matrix and know its (global) dimension, blocking factor, and BLACS context, then you can determine the local problem size by making the call:

\begin{lstlisting}[language=rr]
numroc(dim=dim, bldim=bldim, ICTXT=ICTXT)
\end{lstlisting}

This will return a numeric pair of values, with the first being the number of rows of the local matrix, and the second being the number of columns in the local matrix.  No communication is performed with this call.  However, it is possible that the above can return seemingly nonsensical values.  For example, if a processor owns no piece of the global matrix, then the local dimension information returned from \code{numroc()} could be less than 1 in some dimension (rows, columns, or both).  By default, this should not happen because of an automatic correction, with the smallest return possible being 1.  To allow for the aforementioned possibility, pass the additional argument \code{fixme=FALSE}.

We always make the convention that every processor owns \emph{something}, even if one does not actually own any portion of the global matrix.  The default in this even is a 1 row, 1 column matrix consisting of the single entry $0.0$.  This convention is to prevent problems when passing off data to compiled code (\proglang{C} and \proglang{Fortran}), and care should be taken to preserve this.  As such, the reader may wish to exclusively use \code{numroc()} for its intended purpose (with correction), but may still need to know about the case when the local storage is ``in name only.''  For this, use the \code{ownany()} routine, which answers the question ``does the calling processor own any of the global matrix?'' with a \code{TRUE} (``yes'') or \code{FALSE} (``no'').  The call is virtually identical to \code{numroc()}:

\begin{lstlisting}[language=rr]
ownany(dim=dim, bldim=bldim, ICTXT=ICTXT)
\end{lstlisting}

See the \pkg{pbdBASE} reference manual for full details.